{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Author: Arun Ramesh, University of Houston. https://rpglab.github.io/people/Arun-Venkatesh-Ramesh/\n",
    "#### Source webpage: https://rpglab.github.io/resources/FL-ML-R-SCUC_Python/\n",
    "#### If you use any codes/data here for your work, please cite the following paper: \n",
    "#####       Arun Venkatesh Ramesh and Xingpeng Li, “Feasibility Layer Aided Machine Learning Approach for Day-Ahead Operations”, IEEE Transactions on Power Systems, Apr. 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "np.random.seed(1)\n",
    "\n",
    "import sys\n",
    "nums = np.arange(2000)\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "import csv\n",
    "import time\n",
    "from timeit import default_timer as timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.info of       90.13728903773256  80.95663922833387  75.11440753144379  \\\n",
      "0             67.493145          60.618843          56.244287   \n",
      "1            107.022171          96.121765          89.185142   \n",
      "2            108.926408          97.832052          90.772007   \n",
      "3             62.949194          56.537702          52.457662   \n",
      "4             95.801737          86.044153          79.834781   \n",
      "...                 ...                ...                ...   \n",
      "1494         101.205071          83.534633          79.818072   \n",
      "1495          84.521178          78.343324          74.534879   \n",
      "1496          92.821142          84.133780          89.090335   \n",
      "1497          78.007085          86.473328          79.682256   \n",
      "1498          85.612332          75.013867          67.089895   \n",
      "\n",
      "      128.52909733158162  59.25692149702789  123.5214701628187  \\\n",
      "0              96.240225          44.370493          92.490606   \n",
      "1             152.605688          70.357168         146.660012   \n",
      "2             155.320989          71.609027         149.269522   \n",
      "3              89.760888          41.383267          86.263711   \n",
      "4             136.606181          62.980772         131.283862   \n",
      "...                  ...                ...                ...   \n",
      "1494          123.582270          60.878890         132.604454   \n",
      "1495          125.056675          65.161668         113.900752   \n",
      "1496          143.537284          61.538381         153.068401   \n",
      "1497          133.870249          58.142396         130.036721   \n",
      "1498          125.262158          52.407681         127.618632   \n",
      "\n",
      "      51.74548074388351  70.94138489080802  146.05579242225184  \\\n",
      "0             38.746065          53.119605          109.363892   \n",
      "1             61.438654          84.230412          173.415555   \n",
      "2             62.531827          85.729117          176.501124   \n",
      "3             36.137500          49.543347          102.001009   \n",
      "4             54.997294          75.399516          155.234297   \n",
      "...                 ...                ...                 ...   \n",
      "1494          50.364303          64.814536          151.399639   \n",
      "1495          55.191095          64.646527          128.312405   \n",
      "1496          56.491732          75.023382          155.233330   \n",
      "1497          54.023561          75.286528          140.604006   \n",
      "1498          49.770773          76.869654          161.807403   \n",
      "\n",
      "      83.46045281271535  ...  119.22751544409022.1  75.46045281271533.3  \\\n",
      "0             62.493653  ...             86.099971            54.493653   \n",
      "1             99.094603  ...            143.929472            91.094603   \n",
      "2            100.857785  ...            146.715300            92.857785   \n",
      "3             58.286291  ...             79.452340            50.286291   \n",
      "4             88.705312  ...            127.514394            80.705312   \n",
      "...                 ...  ...                   ...                  ...   \n",
      "1494          83.490475  ...            137.369130            80.431789   \n",
      "1495          79.987625  ...            105.283001            82.767544   \n",
      "1496          88.213839  ...            142.178266            87.802081   \n",
      "1497          73.680902  ...            124.548565            70.530184   \n",
      "1498          90.833218  ...            123.218064            82.663633   \n",
      "\n",
      "      0.163  122.24593355659883.1  67.9144075314438.3  49.04929432826497.1  \\\n",
      "0         0             88.279717           49.044287            35.420874   \n",
      "1         0            147.573256           81.985142            59.211492   \n",
      "2         0            150.429612           83.572007            60.357560   \n",
      "3         0             81.463791           45.257662            32.686089   \n",
      "4         0            130.742606           72.634781            52.458453   \n",
      "...     ...                   ...                 ...                  ...   \n",
      "1494      0            126.811717           70.233824            53.435112   \n",
      "1495      0            124.968589           61.291457            53.878428   \n",
      "1496      0            134.161363           82.170146            52.356536   \n",
      "1497      0            132.826375           61.829803            48.018066   \n",
      "1498      0            118.280322           59.692524            42.836729   \n",
      "\n",
      "      0.164  0.165  0.166  0.167  \n",
      "0         0      0      0      0  \n",
      "1         0      0      0      0  \n",
      "2         0      0      0      0  \n",
      "3         0      0      0      0  \n",
      "4         0      0      0      0  \n",
      "...     ...    ...    ...    ...  \n",
      "1494      0      0      0      0  \n",
      "1495      0      0      0      0  \n",
      "1496      0      0      0      0  \n",
      "1497      0      0      0      0  \n",
      "1498      0      0      0      0  \n",
      "\n",
      "[1499 rows x 576 columns]>\n",
      "<bound method DataFrame.info of       0  0.1  1  1.1  0.2  0.3  1.2  1.3  0.4  0.5  ...  1.407  1.408  1.409  \\\n",
      "0     0    0  1    1    0    0    1    1    0    0  ...      1      1      1   \n",
      "1     0    0  1    1    0    0    1    1    0    0  ...      1      1      1   \n",
      "2     0    0  1    1    0    0    1    1    0    0  ...      1      1      1   \n",
      "3     0    0  0    1    0    0    1    1    0    0  ...      1      1      1   \n",
      "4     0    0  1    1    0    0    1    1    0    0  ...      1      1      1   \n",
      "...  ..  ... ..  ...  ...  ...  ...  ...  ...  ...  ...    ...    ...    ...   \n",
      "1494  0    0  1    1    0    0    1    1    0    0  ...      1      1      1   \n",
      "1495  0    0  1    1    0    0    1    1    0    0  ...      1      1      1   \n",
      "1496  0    0  1    1    0    0    1    1    0    0  ...      1      1      1   \n",
      "1497  0    0  1    1    0    0    1    1    0    0  ...      1      1      1   \n",
      "1498  0    0  1    1    0    0    1    1    0    0  ...      1      1      1   \n",
      "\n",
      "      1.410  1.411  1.412  1.413  1.414  1.415  1.416  \n",
      "0         1      1      1      1      0      0      0  \n",
      "1         1      1      1      1      1      1      1  \n",
      "2         1      1      1      1      1      1      1  \n",
      "3         1      1      1      1      0      0      0  \n",
      "4         1      1      1      1      1      1      1  \n",
      "...     ...    ...    ...    ...    ...    ...    ...  \n",
      "1494      1      1      1      1      1      1      1  \n",
      "1495      1      1      1      1      1      1      1  \n",
      "1496      1      1      1      1      1      1      1  \n",
      "1497      1      1      1      1      1      1      1  \n",
      "1498      1      1      1      1      1      1      1  \n",
      "\n",
      "[1499 rows x 792 columns]>\n",
      "1200\n",
      "1200\n",
      "299\n",
      "299\n",
      "[  91   75 1211  330  961  880  930 1076 1382  335  685  181 1219  101\n",
      "  692 1125  169  924 1093  414 1392 1483 1463 1215  102 1098 1242  920\n",
      " 1197  831 1285  292  759  951  231 1036  746   37   53 1007  923  561\n",
      "  679  589  892  697 1210  893 1220  194 1412 1487  969  484 1041 1163\n",
      "  460 1119  421  596 1467 1404  572  496 1243  304  268  309 1370  535\n",
      "  820  453  921  547  983  772 1054  140  435  937  301 1434  521 1156\n",
      "  527 1453 1138  167  462  390  821  629   48  870  111  874  631  855\n",
      "  386 1284 1395 1425 1133 1167   80  512  636  107  577 1406 1476  812\n",
      "  948  597   60  498  119  375 1240  620  639  876 1245 1381  368 1361\n",
      "  644  868   19 1092  331  201  283  962    3 1091  443  396  553  768\n",
      " 1244 1421 1250   49  587  703  785  104 1165  255 1399  675  223  464\n",
      "  669 1354 1335  563 1336 1010  916   51  372  613 1157  108  653  824\n",
      "  799 1057   30  877 1116  815 1374  538  813  623  258  288  177 1103\n",
      "  729   88  536  531  236 1307 1237  622  776  211  607 1198 1315  382\n",
      " 1341  558  158  766  972  270  996 1293  424  537 1037   94  516 1145\n",
      "  705 1083  737 1318  259  602  955   65  575  359  941  742  661 1134\n",
      " 1170  708  683   56  142 1386 1238 1459  409  654  640 1172  248  267\n",
      "  198 1267  299  204  525 1496  159  227  190  852  584  520  115  321\n",
      "  264  487  858 1229  845 1294  592 1005  906  649   87 1233  418  314\n",
      " 1376  987  428 1102 1262  120 1289  399 1021 1365  186  764 1485  861\n",
      "  133  926 1271   81  638  664 1455 1189 1224  707 1478  755 1141  302\n",
      "  574  786  997 1129   12  408   73  701 1159  556 1090 1204  325 1304\n",
      "  163   47 1208   98  442  718  573  817  280 1440  311  131  935 1281\n",
      "  991 1097 1074  215  135 1231  481 1423  298 1468  426 1194 1146 1328\n",
      " 1470  195 1080 1415 1017 1264 1152  757  341 1445 1045 1410 1435   58\n",
      " 1112  866  239 1452  403  859  748 1209   27  659  593 1059  126 1283\n",
      " 1359  400  851  528  348 1108 1169   90  711 1089  262 1465  495  432\n",
      "  148  336  401  579   78  674  422  857   35  490 1234  823 1447 1323\n",
      "    6 1319  779 1063  819  303 1297  216  599  395  902  350 1387  731\n",
      "  202 1011  383 1196  310  959 1457  404  761    8  571  241  446  491\n",
      " 1369 1177  205  807  966  546 1184  275 1135  452 1060  816  660 1105\n",
      "   62  608  663  315 1048 1313  632 1147  634  503  494  581  154  643\n",
      "  774  827 1015  188  189 1302 1023  478  242 1329  767 1366 1426  351\n",
      "   41 1012 1489 1086   99  286  828  480 1201 1195  771  750  208   83\n",
      "  890  250  936   76  228 1171  612 1409 1253  559 1065 1458 1342 1427\n",
      "  529  777  439  419 1180 1153 1043  323  534 1140 1432  625  682 1317\n",
      "  940  952  200  349  782 1282  860 1291  888  614 1397 1087  628  355\n",
      "  680  493  180  244  804  274  392  220  234   34   85  156  904 1123\n",
      " 1101  363  825  380 1377 1032  509   67 1334  793  698  765  754  471\n",
      "  719  393 1363  899  347  808  329  134 1013    9  957  846   10  647\n",
      "  650  913 1274  284 1035  370  598 1416  125  407 1298 1269  735  541\n",
      "  139  260  473  165  226 1176  191  306  635 1142 1310  482 1206  763\n",
      " 1350  320 1260 1287   17  616  430 1461 1261 1441 1073 1040  207  237\n",
      " 1100  345 1008 1004  340 1251  457  175  549  300  459  709  943 1460\n",
      "  713 1143 1456  702  968  463  978  326  694  730 1109 1473   59   13\n",
      " 1490  841  984  791 1230 1022  603  322  780  441 1450  724  554  256\n",
      "  560  693  700 1493   72  257  985    2 1444 1340  671  367 1124  872\n",
      "  479  977 1268 1038  585  117  945    5 1436  970 1203  950  979  445\n",
      "  136   11 1051  332  604  568  898  725 1437   54  863  233   84  641\n",
      "  894   64 1218  187  853  151 1148  895  388  137  172  953  530  810\n",
      "  455 1173 1131  594 1024 1324  374 1357  909  160 1252   50 1149 1058\n",
      "  670   32  238  834  354  164  486  790  980  881  295 1003  474 1179\n",
      "  472 1009  789  736 1030   97 1068  576  931  720  982  550  308 1047\n",
      "  990  672  918  433  582 1462 1227 1259  839  448 1471  567  619 1183\n",
      "  492  385  741  281  745 1223  483 1192  282  437  830  548 1216 1332\n",
      "  801 1071 1154  673 1428  686 1256 1418  752  121 1185  699  826   16\n",
      " 1480  778 1213 1451 1028  900  293  885   39  106  726   23  710   69\n",
      "  995  425 1020 1316  103  555 1290 1106  927 1403   74  507 1352  218\n",
      " 1322 1070  265  922 1178  434  110  510   40  504  276  809  339 1128\n",
      " 1488  833  185  798 1082  744  925 1296  992   61 1081  578  797  655\n",
      "  993  387  127 1346  251 1286 1006  307  193  646  502   33 1115  800\n",
      " 1117   57  976  358 1182   45 1186 1132  919   68  318  291   46  277\n",
      " 1378  297  224  864 1221 1079 1246  364  427  840  366    0  773  842\n",
      " 1348   66 1205   31  867  891 1144   28  294 1228  113  245  415  769\n",
      "  161  678  411 1166   38 1016 1389    1 1375  247 1371  665 1420 1270\n",
      " 1367   92  381  114 1084  254  429  588  178 1199  942  389  132 1069\n",
      " 1401  570  289  246  929  775  552   89  192 1000 1474 1085  436 1295\n",
      " 1104  651 1492  146  118  994 1438  533  862  802 1014 1066  986 1113\n",
      "  897 1027   26  676   14  500  887   82  353 1273 1408 1484  600  875\n",
      "  273  912  684 1114  466 1373 1486  662  938  447  371  557  854 1053\n",
      "  271  605  449 1276  523  214  344  838 1301  147 1344 1126  723  412\n",
      "  688 1338  783  342  947  747  173  733  377 1029  738  305  543 1064\n",
      "   52   18  285  221 1044 1130   22 1181  116 1026  406  361   95 1225\n",
      "  261 1226 1200  179  716 1413  184  467  903   71 1078  346   29  796\n",
      "  157  932  658 1333  394  988 1424 1055  402  989  677  822  963  835\n",
      " 1033 1241  939  691 1207  334  873  805    7  517  197  296 1094 1331\n",
      "  122  378 1062 1309 1398  162  124  551    4  915  958 1407  611  287\n",
      "  956 1491 1127  908  506  356  352  844  836  946  667  886 1249  333\n",
      "  153  787  501  721  373  590  312   42 1158  645 1272  971 1308  522\n",
      " 1263  762 1266 1118  704 1247 1339  944  168  360 1018 1479  743  272\n",
      "  519  814  818  734 1430 1311 1162 1481 1121  128  998 1330  544  379\n",
      "  203  343 1280 1390  518  488 1472  917  232  794 1402 1122  514 1482\n",
      "  511  112 1433  217  499 1466  934  225 1042 1279 1385  397  540  328\n",
      "  795  848 1443   93  213  770  337  803  212  618 1139 1168 1136   36\n",
      "  105  362  850  756  758  365  856 1257 1495  565  249  476 1188  781\n",
      "  454 1353 1107  230  652   70  610 1174 1439 1394 1454  410 1429 1151\n",
      "  338 1025   43  539  601  965 1343  171  417  837 1248  806  882 1396\n",
      "  637  423  727  967  376  524  438 1292  150 1498 1358  981  954 1349\n",
      "  143  717  526  583  975  566 1232  440 1155  174 1265  123  138  879\n",
      " 1075  732  624  784  182  617  690 1314  240 1254  458  681  465 1187\n",
      " 1191  145  206 1212 1299  324  657 1368  714 1383 1419   79 1364 1351\n",
      "  687  166  229 1448 1325  740  100  290  444  878 1077  739  391   86\n",
      " 1497  666 1275 1347  475   44  788 1002 1137   24 1235  832 1193  591\n",
      " 1475  973 1056  109 1160  222 1052  722   55 1477  253 1422 1072  642\n",
      "  545  477  949  843   63  413  384  974  884  451 1258  656 1327  369\n",
      "  689  219 1400 1320   21  760  869  420  199  450 1161 1019  911  696\n",
      "  811 1236 1469  910 1049  505 1355 1255  405  317  183 1120 1388  609\n",
      "  626  485  278  630   20  170  901 1039  176  327  706 1222  470 1379\n",
      "  266  615  130  865 1442 1362 1384  489  606 1217 1164  416  152  889\n",
      "  849  279 1034  269   96  210  569 1449  871  532 1446 1356 1099  712\n",
      "   77  263 1494  149  469  564  461 1277  896 1393 1345  695 1175  928\n",
      " 1326 1214  586 1372  243 1111  751 1431 1312  648 1190  595  313  933\n",
      "  155 1380 1405 1303 1150  999 1046 1306 1391   25  196 1088   15 1321\n",
      "  883  621 1360 1050 1067  497  792  964  515 1464 1411 1095  542  456\n",
      "  633  431  627  728 1288  209  316  513 1337  829  319 1031 1417  141\n",
      " 1110  753 1001 1239  580  562  398  668  252  907  468  914  357 1278\n",
      " 1300 1202 1305 1414  508  749  129  144  960  847  715  905 1096  235\n",
      " 1061]\n"
     ]
    }
   ],
   "source": [
    "##### data set 24 Bus 8 Prd\n",
    "\n",
    "dfX_24 = pd.read_csv(\"demand24Bus24Prd.txt\")\n",
    "dfY_24 = pd.read_csv(\"commitment24Bus24Prd.txt\")\n",
    "\n",
    "print(dfX_24.info)\n",
    "print(dfY_24.info)\n",
    "\n",
    "x = dfX_24.to_numpy()\n",
    "\n",
    "# normalize X with Base MVA\n",
    "x = x/100\n",
    "#print(x[1])\n",
    "\n",
    "y = dfY_24.to_numpy()\n",
    "#print(y[1])\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger digits.\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 20% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 5\n",
    "(x_train, x_test) = x[:split_at], x[split_at:]\n",
    "(y_train, y_test) = y[:split_at], y[split_at:]\n",
    "\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(y_train))\n",
    "\n",
    "print(len(x_test))\n",
    "print(len(y_test))\n",
    "\n",
    "print(indices)\n",
    "\n",
    "#print(x_train[1])\n",
    "#print(y_train[1])\n",
    "\n",
    "#print(x_test[1])\n",
    "#print(y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 576)\n",
      "(1200, 792)\n",
      "(299, 576)\n",
      "(299, 792)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, nTargets) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    w = np.zeros(shape=(dim,792))\n",
    "    b = 0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Arguments:\n",
    "    z -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1. / ( 1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- trainable weights \n",
    "    b -- bias, a scalar\n",
    "    X -- data normalized demand\n",
    "    Y -- Actual commitment schedules\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w\n",
    "    db -- gradient of the loss with respect to b\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #print(m)\n",
    "    #print(w.shape)\n",
    "    #print(X.shape)\n",
    "    b = np.sum(b) #try b.T\n",
    "        \n",
    "    # Forward propagation (costs)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)             # compute activation\n",
    "    #print(A.shape)\n",
    "    #print(Y.shape)\n",
    "    cost = (-1. / m) * np.sum((Y*np.log(A) + (1 - Y)*np.log(1-A)), axis=1)    # compute cost\n",
    "    #print(cost.shape)\n",
    "    \n",
    "    # Backward Propagartion (gradients)\n",
    "    dw = (1./m)*np.dot(X,((A-Y).T))\n",
    "    db = (1./m)*np.sum(A-Y, axis=1)\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == (792,))\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- trainable weights \n",
    "    b -- bias, a scalar\n",
    "    X -- data normalized demand\n",
    "    Y -- Actual commitment schedules\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 training iterations\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w = w - learning_rate*dw\n",
    "        b = b -  learning_rate*db\n",
    "        \n",
    "        # Record the costs\n",
    "        #if i % 100 == 0:\n",
    "        costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- trainable weights \n",
    "    b -- bias, a scalar\n",
    "    X -- data normalized demand\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions in % for the examples in X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    #Y_prediction = np.zeros((792, m))\n",
    "    w = w.reshape(X.shape[0], 792)\n",
    "    b = np.sum(b)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities commitment\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    #print (A.shape)\n",
    "    #print(A.T)        \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.005, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training input set \n",
    "    Y_train -- training target set\n",
    "    X_test -- test set \n",
    "    Y_test -- test targets\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost for every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "       \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "       \n",
    "    d = {\"costs\": costs,\n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.T\n",
    "y_train = y_train.T\n",
    "\n",
    "x_test = x_test.T\n",
    "y_test = y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 1200)\n",
      "(792, 1200)\n",
      "learning rate is:  0.001\n",
      "training time:  64.32855149999999\n",
      "-------------------------------------------------------\n",
      "learning rate is:  0.003\n",
      "training time:  63.2479374\n",
      "-------------------------------------------------------\n",
      "learning rate is:  0.005\n",
      "training time:  64.9443891\n",
      "-------------------------------------------------------\n",
      "learning rate is:  0.008\n",
      "training time:  64.85721559999999\n",
      "-------------------------------------------------------\n",
      "learning rate is:  0.01\n",
      "training time:  63.42553680000003\n",
      "-------------------------------------------------------\n",
      "learning rate is:  0.03\n",
      "training time:  64.03142860000003\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print (x_train.shape)\n",
    "print (y_train.shape)\n",
    "\n",
    "learning_rates = [0.001, 0.003, 0.005, 0.008, 0.01, 0.03] #0.05 too high\n",
    "\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    start = timer()\n",
    "    print (\"learning rate is: \",i)\n",
    "    models[i] = model(x_train, y_train, x_test, y_test, num_iterations = 1000, learning_rate = i, print_cost = False)\n",
    "    end = timer()\n",
    "    print(\"training time: \",end - start)\n",
    "    print (\"-------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is:  0.001\n",
      "test accuracy: 96.98363230971927 %\n",
      "train accuracy: 97.04850589225589 %\n",
      "-------------------------------------------------------\n",
      "learning rate is:  0.003\n",
      "test accuracy: 97.07780142562751 %\n",
      "train accuracy: 97.08522727272727 %\n",
      "-------------------------------------------------------\n",
      "learning rate is:  0.005\n",
      "test accuracy: 97.32061416844026 %\n",
      "train accuracy: 97.37805134680134 %\n",
      "-------------------------------------------------------\n",
      "learning rate is:  0.008\n",
      "test accuracy: 97.43336373771156 %\n",
      "train accuracy: 97.4709595959596 %\n",
      "-------------------------------------------------------\n",
      "learning rate is:  0.01\n",
      "test accuracy: 97.44941049288876 %\n",
      "train accuracy: 97.4961069023569 %\n",
      "-------------------------------------------------------\n",
      "learning rate is:  0.03\n",
      "test accuracy: 97.21504341069559 %\n",
      "train accuracy: 97.29156144781145 %\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in learning_rates:\n",
    "    print (\"learning rate is: \",i)\n",
    "    w = models[i][\"w\"]\n",
    "    b = models[i][\"b\"]\n",
    "    A_prediction_test = predict(w, b, x_test)\n",
    "    A_prediction_test = A_prediction_test.T\n",
    "    \n",
    "    m = A_prediction_test.shape[0]\n",
    "    Y_test_hackFull = np.zeros((m,A_prediction_test.shape[1]))\n",
    "    P = 0.5\n",
    "\n",
    "    for j in range(m):\n",
    "        for i in range(A_prediction_test.shape[1]):\n",
    "        \n",
    "            # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "            if A_prediction_test[j, i] >= P:\n",
    "                Y_test_hackFull[j, i] = 1\n",
    "            \n",
    "            else:\n",
    "                Y_test_hackFull[j, i] = 0\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_test.T - Y_test_hackFull)) * 100))\n",
    "    \n",
    "    A_prediction_train = predict(w, b, x_train)\n",
    "    A_prediction_train = A_prediction_train.T\n",
    "    \n",
    "    m = A_prediction_train.shape[0]\n",
    "    Y_train_hackFull = np.zeros((m,A_prediction_train.shape[1]))\n",
    "    \n",
    "    for j in range(m):\n",
    "        for i in range(A_prediction_train.shape[1]):\n",
    "        \n",
    "            # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "            if A_prediction_train[j, i] >= P:\n",
    "                Y_train_hackFull[j, i] = 1\n",
    "            \n",
    "            else:\n",
    "                Y_train_hackFull[j, i] = 0\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_train.T - Y_train_hackFull)) * 100))\n",
    "    print (\"-------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is:  0.01\n",
      "test accuracy: 97.44941049288876 %\n",
      "train accuracy: 97.4961069023569 %\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "i = 0.01\n",
    "print (\"learning rate is: \",i)\n",
    "w = models[i][\"w\"]\n",
    "b = models[i][\"b\"]\n",
    "A_prediction_test = predict(w, b, x_test)\n",
    "A_prediction_test = A_prediction_test.T\n",
    "    \n",
    "m = A_prediction_test.shape[0]\n",
    "Y_test_hackFull = np.zeros((m,A_prediction_test.shape[1]))\n",
    "P = 0.5\n",
    "\n",
    "for j in range(m):\n",
    "    for k in range(A_prediction_test.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if A_prediction_test[j, k] >= P:\n",
    "            Y_test_hackFull[j, k] = 1\n",
    "            \n",
    "        else:\n",
    "            Y_test_hackFull[j, k] = 0\n",
    "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_test.T - Y_test_hackFull)) * 100))\n",
    "    \n",
    "A_prediction_train = predict(w, b, x_train)\n",
    "A_prediction_train = A_prediction_train.T\n",
    "    \n",
    "m = A_prediction_train.shape[0]\n",
    "Y_train_hackFull = np.zeros((m,A_prediction_train.shape[1]))\n",
    "    \n",
    "for j in range(m):\n",
    "    for k in range(A_prediction_train.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
    "        if A_prediction_train[j, k] >= P:\n",
    "            Y_train_hackFull[j, k] = 1\n",
    "            \n",
    "        else:\n",
    "            Y_train_hackFull[j, k] = 0\n",
    "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_train.T - Y_train_hackFull)) * 100))\n",
    "print (\"-------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "NCost = int(models[0.01][\"num_iterations\"])#/100)\n",
    "\n",
    "costarray_full = np.zeros((len(learning_rates),NCost))\n",
    "costarray_single = np.zeros(NCost)\n",
    "\n",
    "#print (NCost)\n",
    "for i in range(6):#len(learning_rates)):\n",
    "    costarray_single = np.zeros(NCost)\n",
    "    for j in range(NCost):\n",
    "        #print (j)\n",
    "        costarray_single[j] = np.mean(np.squeeze(models[learning_rates[i]][\"costs\"][j])) \n",
    "    costarray_full[i] = costarray_single\n",
    "    plt.plot(costarray_full[i], label= str(models[learning_rates[i]][\"learning_rate\"]),alpha=0.7)\n",
    "    #print (np.squeeze(models[i][\"costs\"][2]))\n",
    "\n",
    "#plt.plot(costarray_full[i], label= str(models[0.001][\"learning_rate\"]), alpha=1)\n",
    "#plt.plot(np.squeeze(models[0.001][\"costs\"][1]), label= str(models[0.001][\"learning_rate\"]))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations')\n",
    "\n",
    "legend = plt.legend(loc='best', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform comparison with Sklearn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time:  17.11743469999999\n",
      "-------------------------------------------------------\n",
      "training Score:  0.9795717592592592\n",
      "testing Score:  0.9755413668457147\n",
      "train accuracy: 97.95717592592592 %\n",
      "test accuracy: 97.55202526941657 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "#from sklearn.preprocessing import StandardScaler # data normalization\n",
    "#X_var = StandardScaler().fit(X_var).transform(x_train.T)\n",
    "\n",
    "TrainScore = []\n",
    "TestScore = []\n",
    "NgxNt = y_train.shape[0]\n",
    "m_Tr = x_train.shape[1]\n",
    "m_Ts = x_test.shape[1]\n",
    "\n",
    "Y_hat_tr_skln_pred = np.zeros((m_Tr,NgxNt))\n",
    "Y_hat_ts_skln_pred = np.zeros((m_Ts,NgxNt))\n",
    "\n",
    "start = timer()\n",
    "for i in range(NgxNt):\n",
    "    #print (\"Ng*Nt: \",i)   \n",
    "    \n",
    "    if set([0,1]).issubset(set(y_train[i])):      \n",
    "        \n",
    "        #logreg = MultiOutputRegressor(LogisticRegression(multi_class='multinomial', solver='lbfgs'))\n",
    "        logreg = LogisticRegression(random_state=0, solver='liblinear', max_iter = 100)\n",
    "        logreg.fit(x_train.T,y_train[i]) #original shape before transposed\n",
    "        Y_hat_tr_skln_pred[:,i] = logreg.predict(x_train.T)\n",
    "        TrainScore.append(logreg.score(x_train.T,y_train[i]))\n",
    "        Y_hat_ts_skln_pred[:,i] = logreg.predict(x_test.T)\n",
    "        TestScore.append(logreg.score(x_test.T,y_test[i]))\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        if set([0]).issubset(set(y_train[i])):\n",
    "            Y_hat_tr_skln_pred[:,i] = np.zeros(m_Tr)\n",
    "        else:\n",
    "            Y_hat_tr_skln_pred[:,i] = np.ones(m_Tr)\n",
    "        \n",
    "        if set([0]).issubset(set(y_test[i])):\n",
    "            Y_hat_ts_skln_pred[:,i] = np.zeros(m_Ts)\n",
    "        else:\n",
    "            Y_hat_ts_skln_pred[:,i] = np.ones(m_Ts)\n",
    "            \n",
    "        TrainScore.append(1)\n",
    "        TestScore.append(1)\n",
    "end = timer()\n",
    "print(\"training time: \",end - start)\n",
    "print (\"-------------------------------------------------------\")\n",
    "\n",
    "print(\"training Score: \", np.mean(TrainScore))\n",
    "print(\"testing Score: \", np.mean(TestScore))\n",
    "\n",
    "print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_train.T - Y_hat_tr_skln_pred)) * 100))\n",
    "print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_test.T - Y_hat_ts_skln_pred)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_hat_tr_skln_pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is 0.01 \n",
      "training time:  6.479523300000039\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Compare with mltr for same number of iterations\n",
    "\n",
    "start = timer()\n",
    "print (\"learning rate is 0.01 \")\n",
    "models[i] = model(x_train, y_train, x_test, y_test, num_iterations = 100, learning_rate = 0.01, print_cost = False)\n",
    "end = timer()\n",
    "print(\"training time: \",end - start)\n",
    "print (\"-------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20)\n",
      "(100,)\n",
      "[[ 0.6065484   0.81695766  1.05132077 ... -0.36453805  0.16466507\n",
      "  -0.76780375]\n",
      " [-3.05376438  0.92116205 -1.45832446 ...  0.20437739 -1.55269878\n",
      "  -0.4466992 ]\n",
      " [ 0.60640394  0.68064537  1.02124813 ...  1.03703898 -0.83001099\n",
      "  -0.03599018]\n",
      " ...\n",
      " [-2.30803851 -1.42368943  1.14256392 ... -0.24701649 -0.37911961\n",
      "   0.27610275]\n",
      " [-1.53702887  2.14957042  0.32455352 ...  2.15323347  1.31972591\n",
      "  -0.8797298 ]\n",
      " [ 0.37167029 -0.95543218 -0.1484898  ... -0.6294416   0.14225137\n",
      "   0.78002714]]\n",
      "[1 1 1 0 0 0 0 1 0 1 0 0 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 1 0 0 1 1\n",
      " 1 1 1 1 1 0 1 0 0 0 1 1 1 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 1 1\n",
      " 0 1 0 1 0 0 0 0 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=100, random_state=1)\n",
    "\n",
    "print (X.shape)\n",
    "print (y.shape)\n",
    "print (X)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
